
[[filebeat-configuration-details]]
== Configuration

The configuration file of Filebeat uses http://yaml.org/[YAML] as any other Beat.
To start with the configuration it is best to have a look at the default configuration. It is described in detail
what each configuration option is for and how it can be used. Remove the items that are not needed for your setup.

The _etc/filebeat.yml_ consists of the following sections:

* {libbeat}/configuration.html#configuration-shipper[Shipper]
* {libbeat}/configuration.html#configuration-output[Output]
* <<configuration-filebeat-options>>
* {libbeat}/configuration.html#configuration-logging[Logging (optional)]
* {libbeat}/configuration.html#configuration-run-options[Run options (optional)]

[[configuration-filebeat-options]]
=== Filebeat

==== Prospectors

Contains a list of prospector items. Each prospector item is described below.

[source,yaml]
-------------------------------------------------------------------------------------
filebeat:
  prospectors: 1024
-------------------------------------------------------------------------------------

Every prospector item starts with a -. Inside are the prospector specific configs defined
and a list of paths is added that should be crawled. The config fields itself are described
in the config below.

[source,yaml]
-------------------------------------------------------------------------------------
############################# Filbeat ######################################
filebeat:
  # List of prospectors to fetch data.
  prospectors:
    # Each - is a prospector. Below are the prospector specific configurations
    -
      # Paths that should be crawled and fetched. Glob based paths.
      # For each file found under this path, a harvester is started.
      paths:
        - /var/log/*.log
      # - c:\programdata\elasticsearch\logs\*

      # Type of the files. Based on this the way the file is read is decided.
      # The different types cannot be mixed in one prospector
      #
      # Possible options are:
      # * log: Reads every line of the log file (default)
      # * stdin: Reads the standard in
      input_type: log

      # Optional additional fields. These field can be freely picked
      # to add additional information to the crawled log files for filtering
      #fields:
      #  level: debug
      #  review: 1

      # Ignore files which were modified more then the defined timespan in the past
      # Time strings like 2h (2 hours), 5m (5 minutes) can be used.
      #ignore_older:

      # Scan frequency in seconds.
      # How often these files should be checked for changes. In case it is set
      # to 0s, it is done as often as possible. Default: 10s
      #scan_frequency: 10s

      # Defines the buffer size every harvester uses when fetching the file
      #harvester_buffer_size: 16384

      # Setting tail_files to true means filebeat starts readding new files at the end
      # instead of the beginning. If this is used in combination with log rotation
      # this can mean that the first entries of a new file are skipped.
      #tail_files: false

      # Configure the file encoding for reading file with international characters
      # supported encodings:
      #   plain, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk, hzgb2312,
      #   euckr, eucjp, iso2022jp, shiftjis, iso8859-63, iso8859-6i, iso8859-8e,
      #   iso8859-8i
      #encoding: plain
-------------------------------------------------------------------------------------

===== paths

A list of glob based paths that should be crawled and fetched. For each file found under this path, a harvester is
started. Each path is defined one per line.

===== input_type

It has two options:

    * log:  Reads every line of the log file (default)
    * stdin: Reads the standard in

The type value will be put into each event published to Logstash and
Elasticsearch in the 'input_type' field.


===== fields

Add optionally couple of fields to be included to the exported fields by the currently configured
Filebeat instance. These fields can be freely picked to add additional information to the crawled
log files for filtering.

[source,yaml]
-------------------------------------------------------------------------------------
fields:
    level: debug
    review: 1

-------------------------------------------------------------------------------------

===== ignore_older

If set, ignores the files which were modified more then the defined timespan in the past
Time strings like 2h (2 hours), 5m (5 minutes) can be used. Default is set to 10m.

===== scan_frequency

In seconds, specifies how often these files should be checked for changes. In case it is set
to 0s, it is done as often as possible. Default: 10s

===== document_type

Defines the event its type value to be used for published lines read by
harvesters. The document_type will be used by Elasticsearch output as document
type. Default value is `log`.


===== harvester_buffer_size

Defines the buffer size every harvester uses when fetching the file. By default is 16384.


===== tail_files

If this option is set to true, Filebeat starts reading new files at the end of each file instead of the beginning. When this option is used in combination with log rotation, it's possible that the first log entries in a new file might be skipped. The default setting is false.

NOTE: You can use this setting to avoid indexing old log lines when you run Filebeat on a set of log files for the first time. After the first run, we recommend disabling this option, or you risk losing lines during file rotation.

===== spool_size

Event count spool threshold - forces network flush if exceeded.

-------------------------------------------------------------------------------------
filebeat:
  spool_size: 1024
-------------------------------------------------------------------------------------


===== idle_timeout

Defines how often the spooler is flushed. After idle_timeout the spooler is
Flush even though spool_size is not reached. The value must be given as duration string.

[source,yaml]
-------------------------------------------------------------------------------------
filebeat:
  idle_timeout: 5s
-------------------------------------------------------------------------------------



===== registry_file

Name of the registry file. Per default it is put in the current working
directory. In case the working directory is changed after when running
filebeat again, indexing starts from the beginning again.

[source,yaml]
-------------------------------------------------------------------------------------
filebeat:
  registry_file: .filebeat
-------------------------------------------------------------------------------------


===== config_dir

Full Path to directory with additional prospector configuration files. Each file must end with .yml
These config files must have the full filbeat config hierarchy inside, but only
the prospector part is processed. All global options like spool_size are ignored.
The config_dir MUST point to a different directory then where the main filebeat config file is in.

[source,yaml]
-------------------------------------------------------------------------------------
filebeat:
  config_dir: path/to/configs
-------------------------------------------------------------------------------------

===== encoding

Configures the file encoding for reading file with international characters. The
supported encodings are:

    * plain, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk, hzgb2312,
    * euckr, eucjp, iso2022jp, shiftjis, iso8859-63, iso8859-6i, iso8859-8e,
    * iso8859-8i

